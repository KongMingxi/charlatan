
As we have seen, it is possible to learn a meaningful embedding of video frames taken from a single viewpoint onto a vector space. Computer vision has progressed a lot in recent years and methods such as convolutional neural networks have become very usable with tools such as PyTorch. Implementing the neural network and tuning the parameters did not prove to be very difficult.

Learning to imitate was more of a challenge. We performed many runs of our experiments, but ultimately failed to learn any useful behaviour. This might be due to areas in the domain of our embedding function which are not properly mapped onto the codomain. It might be that these areas were not properly represented in our training data.

Proximal policy optimization is known to not be very sample efficient. One hypothesis might be that we simply did not train our policy and value functions for long enough or that we did not manage to find suitable parameters for the algorithm. We can not entirely rule out that there are no bugs in our code. However, we did try our implementation on more simple environments from the Openai gym \citep{gym}. We also tried the PPO algorithm on our simpler goal pose environment. In these simpler environments, the algorithm appeared to learn adequately.

A model-based reinforcement learning algorithm could be quicker to master the task of performing trajectories.

It would be interesting to run our imitation experiments using more computational resources than we had available. Another interesting experiment would be to try and use videos taken from multiple viewpoints.


